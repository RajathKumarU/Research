Effective Data Mining
Using Neural NetworkstitlerpClassification is one of the data mining problems receiving
great attention recently in the database community. This paper
presents an approach to discover symbolic classification rules using
neural networks. Neural networks have not been thought suited for
data mining because how the classifications were made is not explicitly
stated as symbolic rules that are suitable for verification or
interpretation by humans. With the proposed approach, concise
symbolic rules with high accuracy can be extracted from a neural
network. The network is first trained to achieve the required accuracy
rate. Redundant connections of the network are then removed by a
network pruning algorithm. The activation values of the hidden units in
the network are analyzed, and classification rules are generated using
the result of this analysis. The effectiveness of the proposed approach
is clearly demonstrated by the experimental results on a set of
standard data mining test problems.abstractrpData mining, neural networks, rule extraction, network
pruning, classificationkeywordsrpONE of the data mining problems is classification. Various classification
algorithms have been designed to tackle the problem by
researchers in different fields such as mathematical programming,
machine learning, and statistics. Recently, there is a surge of
data mining research in the database community. The classification
problem is re-examined in the context of large databases. Unlike
researchers in other fields, database researchers pay more
attention to the issues related to the volume of data. They are also
concerned with the effective use of the available database techniques,
such as efficient data retrieval mechanisms. With such
concerns, most algorithms proposed are basically based on decision
trees. The general impression is that the neural networks are
not well suited for data mining. The major criticisms include the
following:
1) Neural networks learn the classification rules by many
passes over the training data set so that the learning time of
a neural network is usually long.
2) A neural network is usually a layered graph with the output
of one node feeding into one or many other nodes in the
next layer. The classification process is buried in both the
structure of the graph and the weights assigned to the links
between the nodes. Articulating the classification rules becomes
a difficult problem.
3) For the same reason, available domain knowledge is rather
difficult to be incorporated to a neural network.
On the other hand, the use of neural networks in classification
is not uncommon in machine learning community [5]. In some
cases, neural networks give a lower classification error rate than
the decision trees but require longer learning time [71, [81 In this
paper, we present our results from applying neural networks to mine classification rules for large databases [4] with the focus on
articulating the classification rules represented by neural networks.
The contributions of our study include the following:
Different from previous research work that excludes the
neural network based approaches entirely, we argue that
those approaches should have their position in data mining
because of its merits such as low classification error rates
and robustness to noise.
With our rule extraction algorithms, symbolic classification
rules can be extracted from a neural network. The rules
usually have a comparable classification error rate to those
generated by the decision tree based methods. For a data set
with a strong relationship among attributes, the rules extracted
are generally more concise.
A data mining system based on neural networks is developed.
The system successfully solves a number of classification
problems in the literature.
Our neural network based data mining approach consists of
three major phases:
Network construction and training. This phase constructs and
trains a three layer neural network based on the number of
attributes and number of classes and chosen input coding
method.
Network pruning. The pruning phase aims at removing redundant
links and units without increasing the classification
error rate of the network. A small number of units and links
left in the network after pruning enable us to extract concise
and comprehensible rules.
Rule extraction. This phase extracts the classification rules
from the pruned network. The rules generated are in the
form of "if (a, Bv,) and (x, Bv,) and ... and (x, Bv,) then Cy
where a,s are the attributes of an input tuple, v ,a~re constants,
& are relational operators (=, <, 2, <>), and Ci is one
of the class labels.
Due to space limitation, in this paper we omit the discussion of
the first two phases. Details of these phases can be found in our
earlier work 191, [lo]. We shall elaborate in this paper the third
phase. Section 2 describes our algorithms to extract classification
rules from a neural network and uses an example to illustrate how
the rules are generated using the proposed approach. Section 3
presents some experimental results obtained. Finally, Section 4
concludes the paper.introductionrpIn this paper we present a neural network based approach to
mining classification rules from given databases. The approach
consists of three phases:
1) constructing and training a network to correctly classify tuples in the given training data set to required accuracy,
2) pruning the network while maintaining the classification accuracy, and
3) extracting symbolic rules from the pruned network.
A set of experiments was conducted to test the proposed approach
using a well defined set of data mining problems. The results
indicate that, using the proposed approach, high quality rules can
be discovered from the given data sets.
The work reported here is our attempt to apply the connectionist
approach to data mining to generate rules similar to that of
decision trees. A number of related issues are to be further studied.
One of the issues is to reduce the training time of neural networks.
Although we have been improving the speed of network
training by developing fast algorithms, the time required to extract
rules by our neural network approach is still longer than the time
needed by the decision tree based approach, such as C4.5. As the
long initial training time of a network may be tolerable in some
cases, it is desirable to have incremental training and rule extraction
during the life time of an application database. With an incremental
training that requires less time, the accuracy of rules
extracted can be improved along with the change of database
contents. Another possibility to reduce the training time and improve
the classification accuracy is to reduce the number of input
units of the networks by feature selection.conclusionrpI l l R. Agrawal, T. Imielinski, and A. Swami, ”Database Mining: A
Performance Perspective,” IEEE Trans. Knowledge and Data Eng.,
vol. 5, no. 6, Dec. 1993.
J.E. Dennis Jr. and R.B. Schnabel, Numerical Methods for Unconstrained
Optimization and Nonlinear Equations. Englewood Cliffs,
N.J.: Prentice Hall, 1983.
H. Liu and S.T. Tan, ”X2R A Fast Rule Generator,” Pvoc. IEEE
Int’l Conf. Systems, Man, and Cybernetics. IEEE, 1995.
H. Lu, R. Setiono, and H. Liu, “Neurorule: A Connectionist Approach
to Data Mining,” Proc, VLDB ’95, pp. 478-489,1995.
D. Michie, D.J. Spiegelhalter, and C.C. Taylor, Machine Learning,
Neural and Statistical Classification. Ellis Horwood Series in Artificial
Intelligence, 1994.
J.R. Quinlan, C4.5: Programs for Machine Learning. Morgan Kaufmann,
1993. [7] J.R. Quinlan, ”Comparing Connectionist and Symbolic Learning
Methods,” S.J. Hanson, G.A. Drastall, and R.L. Rivest, eds., Computational
Learning Theory and Natural Learning Systems, vol. 1, pp.
445456. A Bradford Book, MIT Press, 1994.
J.W. Shavlik, R.J. Mooney, and G.G. Towell, ”Symbolic and Neural
Learning Algorithms: An Experimental Comparison,” Machine
Learning, vol. 6, no. 2, pp. 111-143,1991.
[9] R Setiono, “A Neural Network Construction Algorithm which
Maximizes the Likelihood Function,” Connection Science, vol. 7,
no 2, pp. 147-166,1995.
1101 R. Setiono, “A Penalty Function Approach for Pruning Feedforward
Neural Networks,” Neuval Computation, vol. 9, no. 1,
[8]
pp. 301-320,1997.referencesrp